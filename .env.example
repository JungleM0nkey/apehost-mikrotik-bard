# MikroTik Dashboard Configuration

# ==============================================
# MikroTik Router Connection
# ==============================================
MIKROTIK_HOST=192.168.88.1
MIKROTIK_PORT=8728
MIKROTIK_USERNAME=admin
MIKROTIK_PASSWORD=your_password_here

# ==============================================
# Server Configuration
# ==============================================
PORT=3000
NODE_ENV=development
CORS_ORIGIN=http://localhost:5173

# ==============================================
# AI Assistant Configuration
# ==============================================

# LLM Provider Selection
# Options: claude | lmstudio
LLM_PROVIDER=lmstudio

# ----------------------------------------------
# Claude (Anthropic) Configuration
# ----------------------------------------------
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-api03-xxxxx

# Optional: Specify Claude model (default: claude-3-5-sonnet-20241022)
# CLAUDE_MODEL=claude-3-5-sonnet-20241022

# ----------------------------------------------
# LM Studio (Local LLM) Configuration
# ----------------------------------------------
# LM Studio API endpoint (default: http://localhost:1234/v1)
LMSTUDIO_ENDPOINT=http://192.168.100.200:1234

# Model name loaded in LM Studio (required if using LM Studio)
# Example: mistral-7b-instruct-v0.2.Q4_K_M.gguf C:/Users/ilya9/.lmstudio/models/lmstudio-community/Qwen2.5-Coder-14B-Instruct-GGUF/Qwen2.5-Coder-14B-Instruct-Q4_K_M.gguf
LMSTUDIO_MODEL=granite-4.0-h-tiny-Q4_K_M.gguf

# ==============================================
# Setup Instructions
# ==============================================
#
# 1. Copy this file to .env:
#    cp .env.example .env
#
# 2. Configure MikroTik connection details
#
# 3. Choose and configure your AI provider:
#
#    Option A - Claude (Recommended for best quality):
#    - Set LLM_PROVIDER=claude
#    - Get API key from https://console.anthropic.com/
#    - Add your ANTHROPIC_API_KEY
#
#    Option B - LM Studio (Free local alternative):
#    - Download LM Studio from https://lmstudio.ai/
#    - Load a model based on your GPU VRAM (see recommendations below)
#    - Start the local server (default port: 1234)
#    - Set LLM_PROVIDER=lmstudio
#    - Set LMSTUDIO_MODEL to your loaded model name
#
#    Recommended Models by GPU VRAM:
#
#    8GB VRAM (GTX 3060, RTX 4060):
#    - Mistral 7B Instruct (Q4_K_M) - Good balance
#    - Qwen2.5 7B Instruct (Q5_K_M) - Better reasoning
#
#    12GB VRAM (RTX 3060 12GB, RTX 4060 Ti):
#    - Qwen2.5 14B Instruct (Q4_K_M) - Recommended
#    - Mistral 7B Instruct (Q8) - Higher quality 7B
#
#    16GB+ VRAM (RTX 4070 Ti Super, RTX 4080):
#    - Mixtral 8x7B Instruct (Q4_K_M) - HIGHLY RECOMMENDED
#      * Excellent technical reasoning and structured output
#      * Best for MikroTik terminal interactions
#    - Qwen2.5 20B Instruct (Q4_K_M) - Strong alternative
#    - DeepSeek Coder 33B (Q4_K_M) - Specialized for technical tasks
#
#    24GB+ VRAM (RTX 4090, RTX 6000):
#    - Mixtral 8x7B Instruct (Q5_K_M or Q6_K) - Maximum quality
#    - Qwen2.5 32B Instruct (Q4_K_M)
#    - DeepSeek Coder 33B (Q5_K_M)
#
#    Notes:
#    - Q4_K_M = Good balance of quality and speed
#    - Q5_K_M = Higher quality, slightly slower
#    - Mixtral 8x7B is a Mixture-of-Experts model (~47B effective parameters)
#    - For MikroTik CLI parsing, larger models significantly improve accuracy
#
# 4. Restart the server:
#    npm run dev:backend
#
# ==============================================
